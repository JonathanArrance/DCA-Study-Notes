Storage and volumes

State which graph driver should be used on which OS  
Ubuntu - AUFS

Demonstrate how to configure devicemapper  
- devicemapper storage driver is a supported storage driver for Docker EE on many OS distribution. 
- devicemapper is also supported on Docker CE running on CentOS, Fedora, Ubuntu, or Debian.
- Changing the storage driver makes any containers you have already created inaccessible on the local system.
    - Use docker save to save containers, and push existing images to Docker Hub or a private repository, so you do not need to recreate them later.

- Configure device mapper
    - Stop Docker
        - # systemctl stop docker
    - Edit the daemon.json
        - # vi /etc/docker/daemon.json
			{
 				 "storage-driver": "devicemapper"
			}
    - Start docker
        - # systemctl start docker
    - Check if everything is running
        - # docker info

- Production hosts using the devicemapper storage driver must use direct-lvm mode. This mode uses block devices to create the thin pool.

- Configure manually
    - Identify the block device you want to use.
    - Stop docker
    - Install the packages
        - RHEL / CentOS: device-mapper-persistent-data, lvm2, and all dependencies
        - Ubuntu / Debian: thin-provisioning-tools, lvm2, and all dependencies
    - Create a physical volume on your block device from step 1, using the pvcreate command.
- Create a docker volume group on the same device, using the vgcreate command.
- Create two logical volumes named thinpool and thinpoolmeta using the lvcreate command. 
- Convert the volumes to a thin pool and a storage location for metadata for the thin pool, using the lvconvert command.
- Configure autoextension of thin pools via an lvm profile.
    - sudo vi /etc/lvm/profile/docker-thinpool.profile
- Specify thin_pool_autoextend_threshold and thin_pool_autoextend_percent values.
		activation {
  					thin_pool_autoextend_threshold=80
  					thin_pool_autoextend_percent=20
				}
- Apply the LVM profile, using the lvchange command.
    - sudo lvchange --metadataprofile docker-thinpool docker/thinpool
- Enable monitoring for logical volumes on your host.
- If you have ever run Docker on this host before, or if /var/lib/docker/ exists, move it out of the way 
- Edit /etc/docker/daemon.json and configure the options needed for the devicemapper storage driver.
		{
   			 "storage-driver": "devicemapper",
   		 	"storage-opts": [
    			"dm.thinpooldev=/dev/mapper/docker-thinpool",
    			"dm.use_deferred_removal=true",
    			"dm.use_deferred_deletion=true"
    			]
		}
- Start docker
- docker info


Compare object storage to block storage, and explain which one is preferable when available  
- Ideally, very little data is written to a container’s writable layer, and you use Docker volumes to write data. However, some workloads require you to be able to write to the container’s writable layer. This is where storage drivers come in.


- With regard to Docker, the backing filesystem is the filesystem where /var/lib/docker/ is located.
￼


- Workload considerations
    - overlay2, aufs, and overlay all operate at the file level rather than the block level. This uses memory more efficiently, but the container’s writable layer may grow quite large in write-heavy workloads.
    - Block-level storage drivers such as devicemapper, btrfs, and zfs perform better for write-heavy workloads
    - For lots of small writes or containers with many layers or deep filesystems, overlay may perform better than overlay2, but consumes more inodes, which can lead to inode exhaustion.
    - btrfs and zfs require a lot of memory.
    - zfs is a good choice for high-density workloads such as PaaS.


Summarize how an application is composed of layers and where those layers reside on the filesystem  
- Container size on disk
    - size: the amount of data (on disk) that is used for the writable layer of each container.
    - virtual size: the amount of data used for the read-only image data used by the container plus the container’s writable layer size

- Writeable layer
    - very top layer that composes the container
- Image
    - read only lyaer(s) that sit under the writable layer. A running container can share an image with multiple other containers.



Describe how volumes are used with Docker for persistent storage 

- Volumes are the preferred mechanism for persisting data generated by and used by Docker containers.
- Volumes are completely managed by docker

- Advantages over bind mounts
    - easier to backup
    - managed using the cli or API
    - work with both linux and windows containers
    - safer to share among multiple containers
    - volume drivers allow you to store volumes on remote cloud providers, encrypt data, or allow many other functions
    - Now volumes can have content pre-populated by a container

- Start a container mounting a volume
    - docker service create --mount 'type=volume,src=<VOLUME-NAME>,dst=<CONTAINER-PATH>,volume-driver=local,volume-opt=type=nfs,volume-opt=device=<nfs-server>:<nfs-path>,"volume-opt=o=addr=<nfs-address>,vers=4,soft,timeo=180,bg,tcp,rw"' --name myservice <IMAGE>

- The difference between the -v and —mount flags
    - As opposed to bind mounts, all options for volumes are available for both --mount and -v flags.
    - When using volumes with services, only --mount is supported

- backup a container with a volume attached
    - # docker run --rm --volumes-from dbstore -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /dbdata
        - Launch a new container and mount the volume from the dbstore container
        - Mount a local host directory as /backup
        - Pass a command that tars the contents of the dbdata volume to a backup.tar file inside our /backup directory.

- Restore the container
    - # docker run -v /dbdata --name dbstore2 ubuntu /bin/bash
- Then un-tar the backup file in the new container`s data volume:
    - # docker run --rm --volumes-from dbstore2 -v $(pwd):/backup ubuntu bash -c "cd /dbdata && tar xvf /backup/backup.tar --strip 1"


Identify the steps you would take to clean up unused images on a filesystem, also on DTR

Clean up dangling images
- # docker image prune 

Clean up all unused images
- # docker image prune -a

Remove all stopped containers
- # docker container prune

Remove unused volumes
- # docker volume prune

Remove values limit with a filter - by default all unused volumes are removed.
- # docker volume prune --filter "label!=keep"

Remove ununsed network - iptabels are created with every new network so it is a good idea to remove unused networks.
- # docker network prune

The docker system prune command is a shortcut that prunes images, containers, and networks. In Docker 17.06.0 and earlier, volumes are also pruned. In Docker 17.06.1 and higher, you must specify the --volumes flag 

    - # docker system prune —volume

DTR Garbage collection
- Since this process puts DTR in read-only mode and is CPU-intensive, you should run garbage collection jobs outside business peak hours

3 ways to run garbage collection
* Until done: Run the job until all unused image layers are deleted.
* For x minutes: Only run the garbage collection job for a maximum of x minutes at a time.
* Never: Never delete unused image layers.


When garbage collection runs 
- DTR becomes read-only. Images can be pulled, but pushes are not allowed.
- DTR identifies and marks all unused image layers.
- DTR deletes the marked image layers.


Demonstrate how storage can be used across cluster nodes  
Install the proper storage plugin to share storage across cluster nodes
